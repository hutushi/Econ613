---
title: "Econ613 HW2"
author: "Yonghan Shi"
date: "1/30/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	prompt = TRUE
)
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggplot2)
setwd("/Users/whiz/Documents/GitHub/Econ613/Assignments/A2")
```

## Exercise 1 OLS estimate

Consider the following model $$Y =X\beta+\epsilon$$
where X is the age of individuals plus intercept, and Y is the wage.

```{r}
data2009 <- read.csv("Data/datind2009.csv")
data2009 <- data2009 %>%
  select(empstat, wage, age)
```


• Calculate the correlation between Y and X.
```{r}
data1 <- data2009 %>%
  filter(!is.na(wage)) %>%
  filter(wage > 0) %>%
  filter(age >= 18)

x <- data1$age
y <- data1$wage
cor11 <- sum((x-mean(x))*(y-mean(y)))/(sqrt(sum((x-mean(x))^2)*sum((y-mean(y))^2)))
```

The correlation between Y and X is `r cor11`.

• Calculate the coefficients on this regression.

```{r}
X <- cbind(1, data1$age)
beta12 <- solve(t(X) %*% X) %*% (t(X) %*% data1$wage)
rownames(beta12) <- c("intercept", "age")
kable(t(beta12))
```


• Calculate the standard errors of $\beta$

\quad – Using the standard formulas of the OLS.

```{r}
sigsqrt <- sum((y - X %*% beta12)^2)/(nrow(X)-ncol(X))
varcov <- sigsqrt * chol2inv(chol(t(X)%*%X))
stderr <- t(sqrt(diag(varcov)))
table131 <- rbind(c("intercept","age"), stderr)
kable(table131)
```

\quad – Using bootstrap with 49 and 499 replications respectively. Comment on the difference between the two strategies.
```{r}
boot = function(R){
  nind = nrow(data1)
  nvar = 2
  outs = mat.or.vec(R,nvar)
  set.seed(2022)
  for (i in 1:R)
    {
    samp     = sample(1:nind,nind,rep=TRUE)
    dat_samp = data1[samp,]
    Xb = cbind(1, dat_samp$age)
    yb = dat_samp$wage
    outs[i,]     = solve(t(Xb) %*% Xb) %*% (t(Xb) %*% yb)
    }
  sd_est   = apply(outs,2,sd)
  sd_est}

table132 <- rbind(boot(49),boot(499))
rownames(table132) <- c("49 bootstrap","499 bootstrap") 
colnames(table132) <- c("intercept standard errors","age standard error")

kable(table132)
```

With more replications, the standard error will be smaller.

## Exercise 2 Detrend Data

Consider the same application as exercise 1 but using a pooled version of individual data from 2005 to 2018.

```{r}
library(plyr)

filelist <- list.files("Data")

files <- paste("./Data/", filelist, sep = "")

datpool = ldply(files, read_csv)

detach("package:plyr", unload = T)

datpool <- datpool %>%
  select(year, wage, age)
```

• Create a categorical variable ag, which bins the age variables into the following groups: “18-25”, “26-30”, “31-35”, “36-40”,“41-45”, “46-50”,“51-55”, “56-60”, and “60+”.

```{r}
datpool1 <- datpool %>% 
  filter(!is.na(wage)) %>%
  filter(wage > 0) %>%
  filter(age >= 18) %>%
  mutate(ag = as.factor(ifelse(age <= 25, "18-25",
                        ifelse(age <= 30, "26-30",
                        ifelse(age <= 35, "31-35",
                        ifelse(age <= 40, "36-40",
                        ifelse(age <= 45, "41-45",
                        ifelse(age <= 50, "46-50",
                        ifelse(age <= 55, "51-55",
                        ifelse(age <= 60, "56-60", "60+"))))))))))
```

• Plot the wage of each age group across years. Is there a trend?

```{r}
datpool1 <- datpool1 %>%
  mutate(year = as.factor(year))

ggplot(datpool1, aes(x=year, y=wage)) + 
    geom_boxplot(aes(fill = ag))+ ylim(0,150000) + theme_bw()
```

In general the wage peak of age group is moving from the elder to the middle ages. Younger people earn more during the period. 

• Consider $Y_{it} =\beta X_{it} + \gamma_t+ e_{it}$. After including a time fixed effect, how do the estimated coefficients change?

```{r}

reg231 <- lm(datpool1$wage~datpool1$age)
reg232 <- lm(datpool1$wage~ datpool1$age +datpool1$year)

table23 <- rbind(reg231$coefficients, reg232$coefficients[1:2])
rownames(table23) <- c("Original model", "Time fixed effect added")
colnames(table23) <- c("intercept","age")

kable(table23)
```

The coefficients of both age variable and intercept become smaller.

## Exercise 3 Numerical Optimization

We are interested in the effect of age on labor market participation. We consider this problem using the data from 2007. Consider a probit model.

• Exclude all individuals who are inactive.

```{r}
data2007 <- read.csv("Data/datind2007.csv")

#Data cleaning
data3 <- data2007 %>%
  select(wage, age, empstat) %>%
  filter(empstat != "Inactive" & empstat != "Retired") %>%
  mutate(empstat = ifelse(empstat =="Employed", 1, 0))
```


• Write a function that returns the likelihood of the probit of being employed.

You might want to write $X\beta$ first. Then, calculate $F(X\beta)$ and the log likelihood. Remember, for the probit model, $F(x)$ is the standard normal distribution function.

```{r echo=TRUE}
X <- cbind(1, data3$age)
Y <- data3$empstat
  
#negative log-likelihood function
probit.like <- function (beta) {
  Xb <- X %*% beta
  p <- pnorm(Xb)
  -sum((1 - Y) * log(1 - p) + Y * log(p))
}

#gradient function
probit.grad <- function (beta) {
  Xb <- X %*% beta
  p <- pnorm(Xb)
  u <- dnorm(Xb) * (Y - p) / (p * (1 - p))
  -crossprod(X, u)
}

```


• Optimize the model and interpret the coefficients. You can use pre-programmed optimization packages.

```{r}
b <- c(1,0)
fit <- optim(b, probit.like, gr = probit.grad, method = "BFGS", hessian = TRUE)
table33 <- rbind(c("intercept","age"),fit$par)
kable(table33)
```

The coefficients mean that with age becomes older, the probability of getting employed is higher.

• Can you estimate the same model including wages as a determinant of labor market participation? Explain.

I can't. As wage is highly correlated with probability of getting employed. The people who receive no wage would be those who are not employed.

## Exercise 4 Discrete choice

We are interested in the effect of age on labor market participation. Use the pooled version of the data from 2005 to 2015. Additional controls include time-fixed effects.

• Exclude all individuals who are inactive.

```{r}
library(plyr)
filelist2 <- list.files("Data")

filelist2 <- filelist2[1:11]

files2 <- paste("./Data/", filelist2, sep = "")

datpool2 = ldply(files2, read_csv)

detach("package:plyr", unload = T)

datpool2 <- datpool2 %>%
  select(year, age, empstat)

#Data cleaning
data4 <- datpool2 %>%
  filter(empstat != "Inactive" & empstat != "Retired") %>%
  mutate(empstat = ifelse(empstat =="Employed", 1, 0)) %>%
  mutate(year = as.factor(year))

```

• Write and optimize the probit, logit, and the linear probability models.

Remember, for the logit model, $F(x)$ is the logistic function $\dfrac{exp(x)}{(1+exp(x))}$

```{r echo=TRUE}

X1 <- cbind(1, data4$age, model.matrix(~year -1, data4)[,2:11] )
Y1 <- data4$empstat

# probit model

probit.li <- function (beta) {
  est <- X1 %*% beta
  p <- pnorm(est)
  -sum((1 - Y1) * log(1 - p) + Y1 * log(p))
}

probit.gr <- function (beta) {
  est <- X1 %*% beta
  p <- pnorm(est)
  u <- dnorm(est) * (Y1 - p) / (p * (1 - p))
  -crossprod(X1, u)
}

b1 <- c(runif(1,0,1), runif(11,-0.1,0.1))

fit1 <- optim(b1, probit.li, gr=probit.gr, method = "BFGS", hessian = TRUE)

# logit model
logit <- glm(data4$empstat ~ data4$age + data4$year, family = binomial(link = "logit") )

logit.li <- function (beta) {
  est <- X1 %*% beta
  p <- 1/(1+exp(-est))
  -sum((1 - Y1) * log(1 - p) + Y1 * log(p))
}

logit.gr <- function (beta) {
  est <- X1 %*% beta
  p <- 1/(1+exp(-est))
  u <- Y1 - p
  -crossprod(X1, u)
}

b2 <- c(runif(1,0,2), runif(11,-0.1,0.3))

fit2 <- optim(b2, logit.li, gr=logit.gr, method = "BFGS", hessian = TRUE)

# linear probability model
olspar <- solve(t(X1) %*% X1) %*% (t(X1) %*% Y1)
```

• Interpret and compare the estimated coefficients. How significant are they?

```{r}
table4 <- rbind(fit1$par[1:2],fit2$par[1:2],olspar[1:2])
rownames(table4) <- c("probit","logit", "linear")
colnames(table4) <- c("intercept", "age")
kable(table4)
```

The coefficients mean that with age becomes older, the probability of getting employed is higher. The coefficient of logit model is bigger than that of probit model, this is due to the difference $F(x)$ function of the two models.

Based on the Z-ratio of probit model and logit model we have in Excercise 5, we are more than 99% confident to reject the null hypothesis. They are significant. 

## Exercise 5 Marginal Effects
• Compute the marginal effect of the previous probit and logit models.

• Construct the standard errors of the marginal effects. Hint: Boostrap may be the easiest way.

```{r}
mfxboot <- function(func,model,data,boot=99,digits=6){
  x <- glm(func, family=binomial(link=model),data)
  #marginal effects
  pdf <- ifelse(model=="probit",
                mean(dnorm(predict(x, type = "link"))),
                mean(dlogis(predict(x, type = "link"))))
  marginal.effects <- pdf*coef(x)
  # bootstrap
  bootvals <- matrix(rep(NA,boot*length(coef(x))), nrow=boot)
  set.seed(1111)
  for(i in 1:boot){
    samp1 <- data[sample(1:dim(data)[1],replace=T,dim(data)[1]),]
    x1 <- glm(func, family=binomial(link=model),samp1)
    pdf1 <- ifelse(model=="probit",
                   mean(dnorm(predict(x, type = "link"))),
                   mean(dlogis(predict(x, type = "link"))))
    bootvals[i,] <- pdf1*coef(x1)
  }
  res <- cbind(marginal.effects,apply(bootvals,2,sd),marginal.effects/apply(bootvals,2,sd))
  if(names(x$coefficients[1])=="(Intercept)"){
    res1 <- res[2:nrow(res),]
    res2 <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep=""),res1)),nrow=dim(res1)[1])
    rownames(res2) <- rownames(res1)
    } else {
    res2 <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep="")),nrow=dim(res)[1]))
    rownames(res2) <- rownames(res)
    }
  colnames(res2) <- c("marginal.effect","standard.error","z.ratio")  
  return(res2)
}

options(scipen = 200)

mfx1 <- mfxboot(empstat ~ age + year, "probit", data4)
mfxdat1 <- data.frame(cbind(rownames(mfx1),mfx1))

mfx2 <- mfxboot(empstat ~ age + year, "logit", data4)
mfxdat2 <- data.frame(cbind(rownames(mfx2),mfx2))

table5 <- data.frame(rbind(mfxdat1[1,2:4],mfxdat2[1,2:4]))
rownames(table5) <- c("probit", "logit")

kable(table5)
```




